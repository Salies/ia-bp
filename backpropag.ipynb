{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes: número máximo de iterações, erro mínimo e taxa de aprendizado\n",
    "MAX_ITER = 10000\n",
    "MIN_ERROR = 0.01\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "# Função que lê a matriz de entrada de dados, decide número de neurônios de entrada, saída e camada oculta.\n",
    "def read_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    data = data.values\n",
    "    # Número de entradas, ignora a última coluna (que apresenta as classes)\n",
    "    n_inputs = data.shape[1] - 1\n",
    "    # Número de saídas, depende do número de classes únicas na última coluna\n",
    "    n_outputs = len(np.unique(data[:, -1]))\n",
    "    # Número de neurônios na camada oculta é a média geométrica entre o número de entradas e saídas\n",
    "    # Valor é arredondado\n",
    "    n_hidden = int(np.round(np.sqrt(n_inputs * n_outputs)))\n",
    "    return data, (n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "# Função que inicializa os pesos da camada de forma aleatória\n",
    "# Parâmetros utilizados são o número de neurônios da camada anterior e da camada atual\n",
    "def initialize_weights(n_inputs, n_outputs):\n",
    "    # Pesos são inicializados de forma ALEATÓRIA\n",
    "    # Nenhum valor desses gerados aleatoriamente pode ser 0\n",
    "    # Por isso é adicionada uma constante 0.01\n",
    "    #weight = np.random.rand(n_outputs, n_inputs) + 0.01\n",
    "    #weight = np.random.uniform(-0.01, 0.01, (n_outputs, n_inputs)) + 0.001\n",
    "    weight = np.random.normal(scale=0.1, size=(n_outputs, n_inputs))\n",
    "    #print('pesos', weight)\n",
    "    return weight\n",
    "\n",
    "# Função que calcula o valor 'net' de cada neurônio de uma determinada camada\n",
    "# Parâmetros utilizados são os pesos da camada e os valores de entrada\n",
    "def calculate_neurons_net(weights, inputs):\n",
    "    # Multiplica-se os pesos pela entrada\n",
    "    # O resultado é a soma ponderada dos valores de entrada\n",
    "    # O resultado é a entrada para a função de ativação\n",
    "    net = np.dot(weights, inputs)\n",
    "    return net\n",
    "\n",
    "# Função que aplica a função de ativação aos valores 'net' de cada neurônio de uma camada\n",
    "# Parâmetros são os valores 'net' e a funlção de ativação utilizada\n",
    "def apply_transfer_function(net_values, transfer_function):\n",
    "    # Função de ativação é aplicada a cada valor 'net'\n",
    "    # O resultado é o valor de saída de cada neurônio\n",
    "    outputs = transfer_function(net_values)\n",
    "    return outputs\n",
    "\n",
    "# Os valores calculados na função anterior se tornam os inputs utilizados para o cálculo na última camada\n",
    "\n",
    "# Função que calcula o erro de saída de cada neurônio\n",
    "# Parâmetros são os valores de saída e os valores esperados\n",
    "# É necessário utilizar a derivada da função de ativação para o cálculo do erro\n",
    "def calculate_output_error(outputs, expected_outputs, transfer_function_derivative):\n",
    "    # O erro é calculado subtraindo-se os valores de saída dos valores esperados\n",
    "    # O resultado é multiplicado pela derivada da função de ativação\n",
    "    # O resultado é o erro de saída de cada neurônio\n",
    "    error = (expected_outputs - outputs) * transfer_function_derivative(outputs)\n",
    "    return error\n",
    "\n",
    "# Função que propaga o erro para a camada anterior\n",
    "def propagate_error(weights, error, transfer_function_derivative):\n",
    "    # O erro é multiplicado pelos pesos\n",
    "    # O resultado é multiplicado pela derivada da função de ativação\n",
    "    # O resultado é o erro propagado para a camada anterior\n",
    "    error = np.dot(weights.T, error) * transfer_function_derivative(error)\n",
    "    return error\n",
    "\n",
    "# A função anterior é aplicada na ordem inversa da rede, começando pela última camada e depois\n",
    "# sendo aplicada na camada oculta.\n",
    "\n",
    "# Função que ajusta os pesos com base no erro calculado, taxa de aprendizado e valores da função de ativação\n",
    "# Precisa dos valores de saída da camada anterior\n",
    "def adjust_weights(weights, error, last_layer_output):\n",
    "    # O erro é multiplicado pela taxa de aprendizado\n",
    "    # O resultado é multiplicado pelo valor de saída de cada neurônio\n",
    "    # O resultado é o ajuste dos pesos\n",
    "    adjustment = LEARNING_RATE * np.vstack(error) * last_layer_output\n",
    "    # Os pesos são ajustados\n",
    "    weights += adjustment\n",
    "    return weights\n",
    "\n",
    "# Função para normalizaros dados por coluna\n",
    "def normalize_data(data):\n",
    "    # For each column, normalize the numbers to be between 0 and 1\n",
    "    for i in range(data.shape[1] - 1):\n",
    "        # Get the min and max values for the column\n",
    "        min_val = np.min(data[:, i])\n",
    "        max_val = np.max(data[:, i])\n",
    "        # Subtract the min from each value in the column\n",
    "        data[:, i] -= min_val\n",
    "        # Divide each value in the column by the max\n",
    "        data[:, i] /= (max_val - min_val)\n",
    "    return data\n",
    "\n",
    "# Essa função também é aplicada na ordem inversa da rede"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de aplicação das funções elaboradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chegou ao fim pois convergiu dentro do erro mínimo:  0.00885807130802876\n"
     ]
    }
   ],
   "source": [
    "# Função de ativação tangente hiperbólica\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Função de derivada da função de ativação tangente hiperbólica\n",
    "def tanh_derivative(x):\n",
    "    return (1 / np.cosh(x)) ** 2\n",
    "\n",
    "# Função de ativação logística\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Função de derivada da função de ativação logística\n",
    "def logistic_derivative(x):\n",
    "    return logistic(x) * (1 - logistic(x))\n",
    "\n",
    "# Carregando o arquivo \"treinamento.csv\"\n",
    "data, number_of_neurons = read_data('treinamento.csv')\n",
    "\n",
    "# Normalize data to values between 0 and 1\n",
    "data = normalize_data(data.astype(float))\n",
    "\n",
    "# Pesos da camada oculta\n",
    "hidden_layer_weights = initialize_weights(number_of_neurons[0], number_of_neurons[1])\n",
    "# Pesos da camada de saída\n",
    "output_layer_weights = initialize_weights(number_of_neurons[1], number_of_neurons[2])\n",
    "\n",
    "# Manipulando data para que as classes sejam apresentadas de forma alternada\n",
    "# Separando valores por classe\n",
    "data_by_class = {}\n",
    "for row in data:\n",
    "    if row[-1] not in data_by_class:\n",
    "        data_by_class[row[-1]] = np.array([row])\n",
    "    data_by_class[row[-1]] = np.append(data_by_class[row[-1]], [row], axis=0)\n",
    "# Classes\n",
    "classes = list(data_by_class.keys())\n",
    "# Número de classes\n",
    "n_classes = len(classes)\n",
    "# Começa com a primeira classe, depois a segunda, até chegar a última e depois retorna para a primeira\n",
    "# Isso é feito para que a rede aprenda a classificar as classes de forma alternada\n",
    "for itr in range(MAX_ITER):\n",
    "    class_index = itr % n_classes\n",
    "    # Classe atual\n",
    "    current_class = classes[class_index]\n",
    "    # Qual linha da classe atual será escolhida\n",
    "    row_index = itr // n_classes\n",
    "    # Linha atual, com exceção da última coluna, apresenta os inputs\n",
    "    try:\n",
    "        input_from_data = data_by_class[current_class][row_index][:-1]\n",
    "    except IndexError:\n",
    "        # Caso não existir, uma das classes foi esgotada. Para evitar\n",
    "        # desbalanceamento, o treinamento é finalizado.\n",
    "        break\n",
    "\n",
    "    # Calculo de 'net' para a camada oculta\n",
    "    hidden_layer_net = calculate_neurons_net(hidden_layer_weights, input_from_data)\n",
    "    # Aplicação da função de ativação f(x) = x\n",
    "    hidden_layer_outputs = apply_transfer_function(hidden_layer_net, tanh)\n",
    "    \n",
    "    # Calculo de 'net' para a camada de saída\n",
    "    output_layer_net = calculate_neurons_net(output_layer_weights, hidden_layer_outputs)\n",
    "    # Aplicação da função de ativação f(x) = x\n",
    "    output_layer_outputs = apply_transfer_function(output_layer_net, tanh)\n",
    "\n",
    "    # Cálculo do erro de saída\n",
    "    # Os valores esperados são -1 para todas as classes, exceto a atual que será 1\n",
    "    expected_outputs = np.array([-1] * n_classes)\n",
    "    expected_outputs[class_index] = 1\n",
    "    output_layer_error = calculate_output_error(output_layer_outputs, expected_outputs, tanh_derivative)\n",
    "    # Cálculo do erro da camada oculta\n",
    "    hidden_layer_error = propagate_error(output_layer_weights, output_layer_error, tanh_derivative)\n",
    "\n",
    "    # Ajuste de pesos da camada de saída\n",
    "    output_layer_weights = adjust_weights(output_layer_weights, output_layer_error, hidden_layer_outputs)\n",
    "    # Ajuste de pesos da camada oculta\n",
    "    hidden_layer_weights = adjust_weights(hidden_layer_weights, hidden_layer_error, input_from_data)\n",
    "\n",
    "    # Erro total é dado pela soma dos quadrados dos erros de saída divido por 2\n",
    "    total_error = np.sum(output_layer_error ** 2) / 2\n",
    "    #print(total_error)\n",
    "    \n",
    "    # Se o erro total for menor que o erro mínimo, a rede é considerada treinada\n",
    "    if total_error < MIN_ERROR:\n",
    "        print(\"Chegou ao fim pois convergiu dentro do erro mínimo: \", total_error)\n",
    "        break\n",
    "\n",
    "if total_error >= MIN_ERROR:\n",
    "    print(\"Chegou ao fim pois atingiu o número máximo de iterações: \", MAX_ITER)\n",
    "    print(\"Erro total final: \", total_error)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída:\n",
      "[[3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [3.0, 1.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [2.0, 2.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [3.0, 3.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [4.0, 4.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0], [5.0, 5.0]]\n",
      "Erro:  0.1590909090909091\n",
      "Saídas onde a classe esperada é diferente da obtida:\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n",
      "[3.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Testando a rede\n",
    "# Carregando o arquivo \"teste.csv\"\n",
    "data, number_of_neurons = read_data('teste.csv')\n",
    "# Normalize data (all columns except last one) to values between 0 and 1\n",
    "data = normalize_data(data.astype(float))\n",
    "\n",
    "# Número de classes\n",
    "n_classes = len(classes)\n",
    "# Saída\n",
    "output = []\n",
    "# Número de erros\n",
    "n_errors = 0\n",
    "# Número de linhas\n",
    "n_rows = len(data)\n",
    "# Para cada linha do arquivo\n",
    "for row in data:\n",
    "    # Input da linha\n",
    "    input_from_data = row[:-1]\n",
    "    # Classe esperada\n",
    "    expected_class = row[-1]\n",
    "    # Calculo de 'net' para a camada oculta\n",
    "    hidden_layer_net = calculate_neurons_net(hidden_layer_weights, input_from_data)\n",
    "    # Aplicação da função de ativação f(x) = x\n",
    "    hidden_layer_outputs = apply_transfer_function(hidden_layer_net, tanh)\n",
    "    \n",
    "    # Calculo de 'net' para a camada de saída\n",
    "    output_layer_net = calculate_neurons_net(output_layer_weights, hidden_layer_outputs)\n",
    "    # Aplicação da função de ativação f(x) = x\n",
    "    output_layer_outputs = apply_transfer_function(output_layer_net, tanh)\n",
    "\n",
    "    #np.argsort(np.max(x, axis=0))[-2]\n",
    "    # Classe obtida\n",
    "    # make so that the biggest value in output_layer_outputs is 1 and the others are -1\n",
    "    output_layer_outputs = np.where(output_layer_outputs == np.max(output_layer_outputs), 1, -1)\n",
    "\n",
    "    obtained_class = classes[np.argmax(output_layer_outputs)]\n",
    "    # if expected_class is 1.0, use the second highest value~\n",
    "    #if expected_class == 1.0:\n",
    "    #    obtained_class = classes[np.argsort(output_layer_outputs)[-2]]\n",
    "    # Se a classe obtida for igual a esperada, é um acerto\n",
    "    output.append([obtained_class, expected_class])\n",
    "    if obtained_class != expected_class:\n",
    "        n_errors += 1\n",
    "\n",
    "# Imprimindo a saída\n",
    "print(\"Saída:\")\n",
    "print(output)\n",
    "# Imprimindo o erro\n",
    "print(\"Erro: \", n_errors / n_rows)\n",
    "\n",
    "# Print outputs where the expected class is different from the obtained class\n",
    "print(\"Saídas onde a classe esperada é diferente da obtida:\")\n",
    "for row in output:\n",
    "    if row[0] != row[1]:\n",
    "        print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo apresentado na aula 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net da camada oculta:  [-1.4 -4.1  2.5 -1. ]\n",
      "Saída da camada oculta:  [-1.4 -4.1  2.5 -1. ]\n",
      "Net da camada de saída:  [-0.69]\n",
      "Saída da camada de saída:  [-0.69]\n",
      "Erro de saída:  [1.69]\n",
      "Erro da camada oculta:  [2.028 2.704 7.267 5.408]\n",
      "Novos pesos da camada de saída:  [[-1.166 -5.329  8.525  1.51 ]]\n",
      "Novos pesos da camada oculta:  [[ 1.1    0.628]\n",
      " [ 3.6   -1.396]\n",
      " [ 2.1    9.767]\n",
      " [ 0.9    4.408]]\n"
     ]
    }
   ],
   "source": [
    "# Dois neurônios na primeira camada\n",
    "# Quatro neurônios na segunda camada\n",
    "# Três neurônios na terceira camada\n",
    "# Pesos da camada oculta\n",
    "# Taxa de aprendizado igual a 1\n",
    "LEARNING_RATE = 1\n",
    "\n",
    "hidden_layer_weights = np.array([\n",
    "    [1.1, -1.4],\n",
    "    [3.6, -4.1],\n",
    "    [2.1, 2.5],\n",
    "    [0.9, -1.0]])\n",
    "# Pesos da camada de saída\n",
    "output_layer_weights = np.array([\n",
    "    [1.2, 1.6, 4.3, 3.2]\n",
    "])\n",
    "# Entrada é 0, 1\n",
    "input_from_data = np.array([0, 1])\n",
    "# Calculo de 'net' para a camada oculta\n",
    "hidden_layer_net = calculate_neurons_net(hidden_layer_weights, input_from_data)\n",
    "print(\"Net da camada oculta: \", hidden_layer_net)\n",
    "# Aplicação da função de ativação f(x) = x\n",
    "hidden_layer_outputs = apply_transfer_function(hidden_layer_net, lambda x: x)\n",
    "print(\"Saída da camada oculta: \", hidden_layer_outputs)\n",
    "# Calculo de 'net' para a camada de saída\n",
    "output_layer_net = calculate_neurons_net(output_layer_weights, hidden_layer_outputs)\n",
    "print(\"Net da camada de saída: \", output_layer_net)\n",
    "# Aplicação da função de ativação f(x) = x\n",
    "output_layer_outputs = apply_transfer_function(output_layer_net, lambda x: x)\n",
    "print(\"Saída da camada de saída: \", output_layer_outputs)\n",
    "\n",
    "# Cálculo do erro de saída\n",
    "output_layer_error = calculate_output_error(output_layer_outputs, np.array([1]), lambda x: 1)\n",
    "print(\"Erro de saída: \", output_layer_error)\n",
    "# Cálculo do erro da camada oculta\n",
    "hidden_layer_error = propagate_error(output_layer_weights, output_layer_error, lambda x: 1)\n",
    "print(\"Erro da camada oculta: \", hidden_layer_error)\n",
    "\n",
    "# Ajuste de pesos da camada de saída\n",
    "output_layer_weights = adjust_weights(output_layer_weights, output_layer_error, hidden_layer_outputs)\n",
    "print(\"Novos pesos da camada de saída: \", output_layer_weights)\n",
    "# Ajuste de pesos da camada oculta\n",
    "hidden_layer_weights = adjust_weights(hidden_layer_weights, hidden_layer_error, input_from_data)\n",
    "print(\"Novos pesos da camada oculta: \", hidden_layer_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados bem próximos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "679a655ac52b514398ad9d6d6f642a66996226da2671a10b1afca6c75fc972bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
