{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes: número máximo de iterações, erro mínimo e taxa de aprendizado\n",
    "MAX_ITER = 5000\n",
    "MIN_ERROR = 0.01\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que lê a matriz de entrada de dados, decide número de neurônios de entrada, saída e camada oculta.\n",
    "def read_data(csv_file_path):\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    data = data.values\n",
    "    # Número de entradas, ignora a última coluna (que apresenta as classes)\n",
    "    n_inputs = data.shape[1] - 1\n",
    "    # Número de saídas, depende do número de classes únicas na última coluna\n",
    "    n_outputs = len(np.unique(data[:, -1]))\n",
    "    # Número de neurônios na camada oculta é a média geométrica entre o número de entradas e saídas\n",
    "    # Valor é arredondado\n",
    "    n_hidden = int(np.round(np.sqrt(n_inputs * n_outputs)))\n",
    "    return data, (n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "# Função que inicializa os pesos da camada de forma aleatória\n",
    "# Parâmetros utilizados são o número de neurônios da camada anterior e da camada atual\n",
    "def initialize_weights(n_inputs, n_outputs):\n",
    "    # Pesos são inicializados de forma ALEATÓRIA\n",
    "    # Nenhum valor desses gerados aleatoriamente pode ser 0\n",
    "    # Por isso é adicionada uma constante 0.01\n",
    "    weight = np.random.rand(n_outputs, n_inputs) + 0.01\n",
    "    return weight\n",
    "\n",
    "# Função que calcula o valor 'net' de cada neurônio de uma determinada camada\n",
    "# Parâmetros utilizados são os pesos da camada e os valores de entrada\n",
    "def calculate_neurons_net(weights, inputs):\n",
    "    # Multiplica-se os pesos pela entrada\n",
    "    # O resultado é a soma ponderada dos valores de entrada\n",
    "    # O resultado é a entrada para a função de ativação\n",
    "    net = np.dot(weights, inputs)\n",
    "    return net\n",
    "\n",
    "# Função que aplica a função de ativação aos valores 'net' de cada neurônio de uma camada\n",
    "# Parâmetros são os valores 'net' e a funlção de ativação utilizada\n",
    "def apply_transfer_function(net_values, transfer_function):\n",
    "    # Função de ativação é aplicada a cada valor 'net'\n",
    "    # O resultado é o valor de saída de cada neurônio\n",
    "    outputs = transfer_function(net_values)\n",
    "    return outputs\n",
    "\n",
    "# Os valores calculados na função anterior se tornam os inputs utilizados para o cálculo na última camada\n",
    "\n",
    "# Função que calcula o erro de saída de cada neurônio\n",
    "# Parâmetros são os valores de saída e os valores esperados\n",
    "# É necessário utilizar a derivada da função de ativação para o cálculo do erro\n",
    "def calculate_output_error(outputs, expected_outputs, transfer_function_derivative):\n",
    "    # O erro é calculado subtraindo-se os valores de saída dos valores esperados\n",
    "    # O resultado é multiplicado pela derivada da função de ativação\n",
    "    # O resultado é o erro de saída de cada neurônio\n",
    "    error = (expected_outputs - outputs) * transfer_function_derivative(outputs)\n",
    "    return error\n",
    "\n",
    "# Função que propaga o erro para a camada anterior\n",
    "def propagate_error(weights, error, transfer_function_derivative):\n",
    "    # O erro é multiplicado pelos pesos\n",
    "    # O resultado é multiplicado pela derivada da função de ativação\n",
    "    # O resultado é o erro propagado para a camada anterior\n",
    "    error = np.dot(weights.T, error) * transfer_function_derivative(error)\n",
    "    return error\n",
    "\n",
    "# A função anterior é aplicada na ordem inversa da rede, começando pela última camada e depois\n",
    "# sendo aplicada na camada oculta.\n",
    "\n",
    "# Função que ajusta os pesos com base no erro calculado, taxa de aprendizado e valores da função de ativação\n",
    "# Precisa dos valores de saída da camada anterior\n",
    "def adjust_weights(weights, error, last_layer_output):\n",
    "    # O erro é multiplicado pela taxa de aprendizado\n",
    "    # O resultado é multiplicado pelo valor de saída de cada neurônio\n",
    "    # O resultado é o ajuste dos pesos\n",
    "    adjustment = LEARNING_RATE * np.vstack(error) * last_layer_output\n",
    "    # Os pesos são ajustados\n",
    "    weights += adjustment\n",
    "    return weights\n",
    "\n",
    "# Essa função também é aplicada na ordem inversa da rede"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de aplicação das funções elaboradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4688636418889134\n",
      "0.3689630085862306\n",
      "0.4000739835590875\n",
      "1.550861477177638\n",
      "0.36834672489281634\n",
      "0.4412188930413208\n",
      "0.5812720858190745\n",
      "1.767489224906561\n",
      "0.42961409501091835\n",
      "2.7544502517971914\n",
      "0.5999108277122689\n",
      "0.7004488239107345\n",
      "1.0018298291130128\n",
      "1.4302850048725364\n",
      "0.5511255722507363\n",
      "1.8697154052066332\n",
      "1.2679457852856497\n",
      "0.7282784477950708\n",
      "0.4532741939140372\n",
      "0.661133584933711\n",
      "1.695575622548724\n",
      "1.3763158747680548\n",
      "1.2204601287333265\n",
      "1.3105875999322978\n",
      "0.5226426896093761\n",
      "1.417848954478449\n",
      "1.9525184242256661\n",
      "0.4370090240858179\n",
      "0.4883344472872548\n",
      "0.480313086095551\n",
      "1.3099312959398037\n",
      "1.6751093855279788\n",
      "0.8651516359496457\n",
      "0.665432016409439\n",
      "0.7758215327706481\n",
      "0.46145326604586423\n",
      "0.5246621437795024\n",
      "0.5079105090731731\n",
      "0.5215964779365155\n",
      "0.5548106014236509\n",
      "0.8974621393114\n",
      "0.5360949225060752\n",
      "1.2254537560113332\n",
      "1.4143748543047008\n",
      "0.7652923840920242\n",
      "0.4832263365078729\n",
      "1.0596246345702474\n",
      "0.4869863345274926\n",
      "0.5764905599945522\n",
      "0.9092174859172264\n",
      "0.596776164247783\n",
      "1.2692194559624563\n",
      "1.2819818930210962\n",
      "0.6871540136694868\n",
      "0.7319516743463806\n",
      "1.5601270151659494\n",
      "0.6247032688642912\n",
      "1.5066218205846065\n",
      "0.27454328727097094\n",
      "0.6266217228368911\n",
      "1.7397081283790004\n",
      "0.5418611512943156\n",
      "1.1970675087571592\n",
      "0.32221050314883143\n",
      "0.5985907703287033\n",
      "0.6724754866873747\n",
      "1.3131369881666117\n",
      "0.74582762029566\n",
      "0.13009921700033425\n",
      "1.4588188611373618\n",
      "0.6960728665248943\n",
      "1.2696842592646538\n",
      "1.134460033795435\n",
      "1.449420991155868\n",
      "0.6800444490066295\n",
      "1.5936598516909555\n",
      "0.6917357904090724\n",
      "1.022729522400962\n",
      "0.1883757658365119\n",
      "0.7224270146548702\n",
      "1.4137094417497742\n",
      "1.0412168918694293\n",
      "1.1933775945777154\n",
      "0.6033711728837713\n",
      "0.7234706806470819\n",
      "1.4573708242353143\n",
      "1.6579020206571347\n",
      "0.8790906229098282\n",
      "0.2025353901629918\n",
      "0.7627337833874236\n",
      "1.4728625185602997\n",
      "1.0215494166996553\n",
      "1.0379309959674992\n",
      "0.2725361107526624\n",
      "0.7287873282897184\n",
      "1.147333877890823\n",
      "1.3443679588640949\n",
      "0.8231335600569394\n",
      "1.3487505074244261\n",
      "1.3586718557522124\n",
      "0.7442399880767697\n",
      "0.9252518491752918\n",
      "1.3972435872137363\n",
      "0.42396272586007433\n",
      "0.739091385135899\n",
      "1.3955186046426122\n",
      "1.0205470469723723\n",
      "0.8430158791168276\n",
      "0.37709240805189426\n",
      "0.7124658392307704\n",
      "1.2788966187002553\n",
      "1.3647319362881405\n",
      "0.9987711397050227\n",
      "1.3528354681670547\n",
      "0.7173692291257543\n",
      "0.8645087328848341\n",
      "0.989629835832712\n",
      "1.5876731430626498\n",
      "0.5363845336994764\n",
      "0.7512452561522555\n",
      "1.1634779688324464\n",
      "1.1642442021616115\n",
      "1.242063689038308\n",
      "0.5271152063405754\n",
      "0.7237018593874885\n",
      "1.3210689076813038\n",
      "0.7754991929991222\n",
      "1.3212327823571666\n",
      "1.4021609803970172\n",
      "0.6539129577900532\n",
      "0.6830920237647423\n",
      "1.3170988002100932\n",
      "0.5706235374573067\n",
      "0.7792367563704722\n",
      "0.5704244791286398\n",
      "0.1951348427053186\n",
      "1.1214996052237565\n",
      "0.868257744373254\n",
      "0.20562343463238675\n",
      "1.287927777786737\n",
      "1.017009521621996\n",
      "0.5851015898278249\n",
      "0.7671250606787339\n",
      "0.10195408414556073\n",
      "0.7207754484969301\n",
      "1.3468352688202374\n",
      "1.2422983696985215\n",
      "0.2420824081476065\n",
      "0.5667862923198276\n",
      "0.6470276433899398\n",
      "0.8326870415430565\n",
      "1.2877596589571576\n",
      "1.1576929416219781\n",
      "0.9379207719991478\n",
      "0.7260665185478886\n",
      "1.4293670313455236\n",
      "1.3253867483979047\n",
      "0.11664861633940364\n",
      "0.16784509733701786\n",
      "0.6116964963657503\n",
      "0.8860285333188742\n",
      "1.5254918237447495\n",
      "1.4276539278393308\n",
      "0.09020961448470538\n",
      "0.7094023025724246\n",
      "1.2418489821820913\n",
      "0.9171942645962174\n",
      "0.11908984705409681\n",
      "0.05601798213839094\n",
      "0.703405789878365\n",
      "0.048495504688248675\n",
      "0.7982003495651148\n",
      "0.1605343151094576\n",
      "0.1457166197016786\n",
      "0.7326352783956535\n",
      "0.46225617358317217\n",
      "1.0586020210787859\n",
      "1.3688984186643296\n",
      "0.07944960917694524\n",
      "0.6321818955166164\n",
      "0.06312641004595602\n",
      "0.6340998477433403\n",
      "0.23757672023180867\n",
      "0.15846916539453176\n",
      "0.3107851626767938\n",
      "0.9157514667356004\n",
      "1.2284305923971806\n",
      "0.13750438947039031\n",
      "1.1910463684943016\n",
      "0.041345685073218015\n",
      "0.013875464862035032\n",
      "0.5685004724090084\n",
      "1.4199672172540496\n",
      "0.2572128835035922\n",
      "1.3192989180408143\n",
      "0.5364466335063821\n",
      "0.8013776157864152\n",
      "0.9208406507783824\n",
      "1.3283537429521222\n",
      "0.08441558847595693\n",
      "1.2716718183168398\n",
      "1.1961732918447145\n",
      "0.045856059156242975\n",
      "0.5553489643964243\n",
      "0.045345896873907963\n",
      "0.7293748261956026\n",
      "0.5229622983897166\n",
      "0.05023027297864419\n",
      "0.1767378477546651\n",
      "0.10012550642601285\n",
      "0.2277240716036731\n",
      "0.585941177100284\n",
      "0.05030229342383988\n",
      "0.10065669189014745\n",
      "0.17755572409194745\n",
      "1.3895448701581525\n",
      "0.860161366515974\n",
      "0.08063949336557086\n",
      "0.20908539337312806\n",
      "0.16200524805127417\n",
      "0.054322855731007855\n",
      "0.7152151602034995\n",
      "1.1504638041956823\n",
      "1.3998406767239047\n",
      "0.1339395851047176\n",
      "1.0769946727266513\n",
      "1.0936851846661129\n",
      "0.027827183554490098\n",
      "0.32270189490967044\n",
      "0.21682067887028303\n",
      "0.8772066970418753\n",
      "1.573843507312344\n",
      "0.02449230753796937\n",
      "0.1438920863598183\n",
      "0.44564812245661994\n",
      "0.20779748518061378\n",
      "0.6557107127838981\n",
      "0.027559285884429473\n",
      "0.09335227627886644\n",
      "0.27165647767444234\n",
      "0.09915845861025414\n",
      "0.7376974542407634\n",
      "0.03122701899420243\n",
      "1.3833593387801832\n",
      "0.19943280819344877\n",
      "1.2627115369590496\n",
      "1.3903380383397907\n",
      "0.03635717316157754\n",
      "0.014483119664425228\n",
      "0.9865012254310159\n",
      "0.007663291974920496\n",
      "Chegou ao fim pois convergiu dentro do erro mínimo:  0.007663291974920496\n"
     ]
    }
   ],
   "source": [
    "# Função de ativação tangente hiperbólica\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Função de derivada da função de ativação tangente hiperbólica\n",
    "def tanh_derivative(x):\n",
    "    return (1 / np.cosh(x)) ** 2\n",
    "\n",
    "# Carregando o arquivo \"treinamento.csv\"\n",
    "data, number_of_neurons = read_data('treinamento.csv')\n",
    "# Pesos da camada oculta\n",
    "hidden_layer_weights = initialize_weights(number_of_neurons[0], number_of_neurons[1])\n",
    "# Pesos da camada de saída\n",
    "output_layer_weights = initialize_weights(number_of_neurons[1], number_of_neurons[2])\n",
    "\n",
    "# Manipulando data para que as classes sejam apresentadas de forma alternada\n",
    "# Separando valores por classe\n",
    "data_by_class = {}\n",
    "for row in data:\n",
    "    if row[-1] not in data_by_class:\n",
    "        data_by_class[row[-1]] = np.array([row])\n",
    "    data_by_class[row[-1]] = np.append(data_by_class[row[-1]], [row], axis=0)\n",
    "# Classes\n",
    "classes = list(data_by_class.keys())\n",
    "# Número de classes\n",
    "n_classes = len(classes)\n",
    "# Começa com a primeira classe, depois a segunda, até chegar a última e depois retorna para a primeira\n",
    "# Isso é feito para que a rede aprenda a classificar as classes de forma alternada\n",
    "for itr in range(MAX_ITER):\n",
    "    class_index = itr % n_classes\n",
    "    # Classe atual\n",
    "    current_class = classes[class_index]\n",
    "    # Qual linha da classe atual será escolhida\n",
    "    row_index = itr // n_classes\n",
    "    # Linha atual, com exceção da última coluna, apresenta os inputs\n",
    "    try:\n",
    "        input_from_data = data_by_class[current_class][row_index][:-1]\n",
    "    except IndexError:\n",
    "        # Caso não existir, uma das classes foi esgotada. Para evitar\n",
    "        # desbalanceamento, o treinamento é finalizado.\n",
    "        break\n",
    "\n",
    "\n",
    "    # Calculo de 'net' para a camada oculta\n",
    "    hidden_layer_net = calculate_neurons_net(hidden_layer_weights, input_from_data)\n",
    "    # Aplicação da função de ativação f(x) = x\n",
    "    hidden_layer_outputs = apply_transfer_function(hidden_layer_net, tanh)\n",
    "    \n",
    "    # Calculo de 'net' para a camada de saída\n",
    "    output_layer_net = calculate_neurons_net(output_layer_weights, hidden_layer_outputs)\n",
    "    # Aplicação da função de ativação f(x) = x\n",
    "    output_layer_outputs = apply_transfer_function(output_layer_net, tanh)\n",
    "\n",
    "    # Cálculo do erro de saída\n",
    "    # Os valores esperados são -1 para todas as classes, exceto a atual que será 1\n",
    "    expected_outputs = np.array([-1] * n_classes)\n",
    "    expected_outputs[class_index] = 1\n",
    "    output_layer_error = calculate_output_error(output_layer_outputs, expected_outputs, tanh_derivative)\n",
    "    # Cálculo do erro da camada oculta\n",
    "    hidden_layer_error = propagate_error(output_layer_weights, output_layer_error, tanh_derivative)\n",
    "\n",
    "    # Ajuste de pesos da camada de saída\n",
    "    output_layer_weights = adjust_weights(output_layer_weights, output_layer_error, hidden_layer_outputs)\n",
    "    # Ajuste de pesos da camada oculta\n",
    "    hidden_layer_weights = adjust_weights(hidden_layer_weights, hidden_layer_error, input_from_data)\n",
    "\n",
    "    # Erro total é dado pela soma dos quadrados dos erros de saída divido por 2\n",
    "    total_error = np.sum(output_layer_error ** 2) / 2\n",
    "    print(total_error)\n",
    "    \n",
    "    # Se o erro total for menor que o erro mínimo, a rede é considerada treinada\n",
    "    if total_error < MIN_ERROR:\n",
    "        print(\"Chegou ao fim pois convergiu dentro do erro mínimo: \", total_error)\n",
    "        break\n",
    "\n",
    "if total_error >= MIN_ERROR:\n",
    "    print(\"Chegou ao fim pois atingiu o número máximo de iterações: \", MAX_ITER)\n",
    "    print(\"Erro total final: \", total_error)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo apresentado na aula 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net da camada oculta:  [-1.4 -4.1  2.5 -1. ]\n",
      "Saída da camada oculta:  [-1.4 -4.1  2.5 -1. ]\n",
      "Net da camada de saída:  [-0.69]\n",
      "Saída da camada de saída:  [-0.69]\n",
      "Erro de saída:  [1.69]\n",
      "Erro da camada oculta:  [2.028 2.704 7.267 5.408]\n",
      "Novos pesos da camada de saída:  [[-1.166 -5.329  8.525  1.51 ]]\n",
      "Novos pesos da camada oculta:  [[ 1.1    0.628]\n",
      " [ 3.6   -1.396]\n",
      " [ 2.1    9.767]\n",
      " [ 0.9    4.408]]\n"
     ]
    }
   ],
   "source": [
    "# Dois neurônios na primeira camada\n",
    "# Quatro neurônios na segunda camada\n",
    "# Três neurônios na terceira camada\n",
    "# Pesos da camada oculta\n",
    "# Taxa de aprendizado igual a 1\n",
    "LEARNING_RATE = 1\n",
    "\n",
    "hidden_layer_weights = np.array([\n",
    "    [1.1, -1.4],\n",
    "    [3.6, -4.1],\n",
    "    [2.1, 2.5],\n",
    "    [0.9, -1.0]])\n",
    "# Pesos da camada de saída\n",
    "output_layer_weights = np.array([\n",
    "    [1.2, 1.6, 4.3, 3.2]\n",
    "])\n",
    "# Entrada é 0, 1\n",
    "input_from_data = np.array([0, 1])\n",
    "# Calculo de 'net' para a camada oculta\n",
    "hidden_layer_net = calculate_neurons_net(hidden_layer_weights, input_from_data)\n",
    "print(\"Net da camada oculta: \", hidden_layer_net)\n",
    "# Aplicação da função de ativação f(x) = x\n",
    "hidden_layer_outputs = apply_transfer_function(hidden_layer_net, lambda x: x)\n",
    "print(\"Saída da camada oculta: \", hidden_layer_outputs)\n",
    "# Calculo de 'net' para a camada de saída\n",
    "output_layer_net = calculate_neurons_net(output_layer_weights, hidden_layer_outputs)\n",
    "print(\"Net da camada de saída: \", output_layer_net)\n",
    "# Aplicação da função de ativação f(x) = x\n",
    "output_layer_outputs = apply_transfer_function(output_layer_net, lambda x: x)\n",
    "print(\"Saída da camada de saída: \", output_layer_outputs)\n",
    "\n",
    "# Cálculo do erro de saída\n",
    "output_layer_error = calculate_output_error(output_layer_outputs, np.array([1]), lambda x: 1)\n",
    "print(\"Erro de saída: \", output_layer_error)\n",
    "# Cálculo do erro da camada oculta\n",
    "hidden_layer_error = propagate_error(output_layer_weights, output_layer_error, lambda x: 1)\n",
    "print(\"Erro da camada oculta: \", hidden_layer_error)\n",
    "\n",
    "# Ajuste de pesos da camada de saída\n",
    "output_layer_weights = adjust_weights(output_layer_weights, output_layer_error, hidden_layer_outputs)\n",
    "print(\"Novos pesos da camada de saída: \", output_layer_weights)\n",
    "# Ajuste de pesos da camada oculta\n",
    "hidden_layer_weights = adjust_weights(hidden_layer_weights, hidden_layer_error, input_from_data)\n",
    "print(\"Novos pesos da camada oculta: \", hidden_layer_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados bem próximos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5b74985009ca374bd515082ff5cf4499f91afda439f67dd1052b82b1765b1c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
